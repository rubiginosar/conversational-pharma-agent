{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b9b58dc",
   "metadata": {},
   "source": [
    "classification with a fine-tuned model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bc75a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cartesinus/multilingual_minilm-amazon_massive-intent_eu7 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([60]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([60, 384]) in the checkpoint and torch.Size([3, 384]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b4d0aa5ac1b4d9fbd68a5a529fb57d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/314 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4113da37bb824750bc8dbca06ed3badb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/79 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\THINKPAD T470\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\THINKPAD T470\\AppData\\Local\\Temp\\ipykernel_22344\\3326029631.py:62: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 32/200 1:38:32 < 9:11:50, 0.01 it/s, Epoch 0.78/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, pipeline\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Conversion en un objet Dataset compatible avec Hugging Face.\n",
    "with open(\"jsons/enriched_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "texts = [item[\"text\"] for item in raw_data[\"dataset\"]]\n",
    "labels = [item[\"label\"] for item in raw_data[\"dataset\"]]\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": texts, \"label\": labels})\n",
    "\n",
    "# Step 2: Load tokenizer and model\n",
    "model_name = \"cartesinus/multilingual_minilm-amazon_massive-intent_eu7\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=3,  # label 0: suivi, label 1: r√©clamation, label 2: commande\n",
    "    ignore_mismatched_sizes=True \n",
    "    # ignore_mismatched_sizes=True permet d‚Äôajuster automatiquement la derni√®re couche de classification.\n",
    ") \n",
    "\n",
    "# Step 3: Define tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "# Convertit les textes en tokens utilisables par le mod√®le, avec padding/truncation √† une longueur fixe de 128.\n",
    "\n",
    "# Step 4: Split the dataset into train and test sets\n",
    "# S√©paration du jeu de donn√©es en 80% entra√Ænement et 20% et tokenization des 2\n",
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = split_dataset[\"train\"].map(tokenize_function, batched=True)\n",
    "eval_dataset = split_dataset[\"test\"].map(tokenize_function, batched=True)\n",
    "\n",
    "# Step 5: Define TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_model_11\",      # Save directory\n",
    "    evaluation_strategy=\"epoch\",             # Evaluate after every epoch\n",
    "    learning_rate=2e-5,                      # Learning rate\n",
    "    per_device_train_batch_size=8,           # Train batch size\n",
    "    per_device_eval_batch_size=8,            # Eval batch size\n",
    "    num_train_epochs=5,                      # Number of epochs\n",
    "    weight_decay=0.01,                       # Weight decay\n",
    "    save_strategy=\"epoch\",                   # Save after each epoch\n",
    "    logging_dir=\"./logs\",                    # Logging directory\n",
    "    logging_steps=1,                         # Log every step\n",
    "    load_best_model_at_end=True,             # Load best model\n",
    "    metric_for_best_model=\"accuracy\",        # Monitor accuracy\n",
    "    greater_is_better=True,                  # Higher accuracy is better\n",
    "    report_to=\"none\",                        # Disable external logging\n",
    ")\n",
    "\n",
    "# Step 6: Define compute_metrics function\n",
    "# Compare les pr√©dictions du mod√®le aux vraies √©tiquettes\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1\": f1_score(labels, predictions, average=\"weighted\")\n",
    "    }\n",
    "\n",
    "# Step 7: Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Step 8: Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Step 9: Save the fine-tuned model and tokenizer\n",
    "trainer.save_model(\"./fine_tuned_model_11\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model_11\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d545a0e",
   "metadata": {},
   "source": [
    "Loading and using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b44d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\THINKPAD T470\\AppData\\Roaming\\Python\\Python310\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.638332188129425}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# pipeline(\"text-classification\", ...) charge automatiquement le tokenizer et le mod√®le.\n",
    "classifier = pipeline(\"text-classification\", model=\"./fine_tuned_model_11\")\n",
    "text= \"ou est mon livreur?\"\n",
    "result= classifier(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf09d9a",
   "metadata": {},
   "source": [
    "Classification with a RAG-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "259fd576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Load dataset\n",
    "with open('enriched_dataset.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "texts = [item['text'] for item in data['dataset']]\n",
    "labels = [item['label'] for item in data['dataset']]\n",
    "\n",
    "# Label mapping\n",
    "label_names = {\n",
    "    0: \"Suivi (Tracking)\",\n",
    "    1: \"R√©clamation (Complaint)\",\n",
    "    2: \"Commande (Order)\",\n",
    "    4: \"Ambigu√´ (Ambiguous)\"\n",
    "}\n",
    "\n",
    "# Load embedding model\n",
    "# Utilisation d‚Äôun mod√®le multilingue pour transformer chaque phrase en vecteur dense de taille fixe.\n",
    "# Ces vecteurs seront utilis√©s pour mesurer la similarit√©.\n",
    "embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "text_embeddings = embedding_model.encode(texts)\n",
    "\n",
    "# Fit Nearest Neighbors\n",
    "# Entra√Ænement d‚Äôun mod√®le k-NN sur les embeddings.\n",
    "# On utilisera les 3 voisins les plus proches pour classifier une nouvelle phrase.\n",
    "n_neighbors = 3\n",
    "nn = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "nn.fit(text_embeddings)\n",
    "\n",
    "# Classifier function with output\n",
    "# Fonction qui prend une phrase en entr√©e, g√©n√®re son embedding, et la compare aux voisins.\n",
    "def classify_input(text, ambiguity_threshold=0.15):\n",
    "    # Calcule l‚Äôembedding du texte d‚Äôentr√©e, Cherche les k voisins les plus proches et R√©cup√®re leurs labels.\n",
    "    embedding = embedding_model.encode([text])\n",
    "    distances, indices = nn.kneighbors(embedding)\n",
    "    neighbor_labels = np.array(labels)[indices[0]]\n",
    "\n",
    "# Calcule la distribution des labels dans les voisins pour estimer une probabilit√© par intention (probabilit√©s d‚Äôintention)\n",
    "    counts = np.bincount(neighbor_labels, minlength=3)\n",
    "    probs = counts / n_neighbors\n",
    "    top_probs = np.sort(probs)\n",
    "\n",
    "    # Ambiguity check\n",
    "    # Si les 3 meilleures probabilit√©s sont trop proches, la requ√™te est jug√©e ambigu√´.\n",
    "    # Cela emp√™che de faire des pr√©dictions trop incertaines.\n",
    "    if top_probs[-1] - top_probs[-2] < ambiguity_threshold and top_probs[-2] - top_probs[-3] < ambiguity_threshold:\n",
    "        predicted_label = 4\n",
    "        confidence = None\n",
    "    else:\n",
    "        predicted_label = int(np.argmax(probs))\n",
    "        confidence = probs[predicted_label]\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"Input: '{text}'\")\n",
    "    if predicted_label == 4:\n",
    "        print(\"Predicted: Ambigu√´ (Ambiguous)\")\n",
    "    else:\n",
    "        print(f\"Predicted: {label_names[predicted_label]} (confidence: {confidence * 100:.1f}%)\")\n",
    "\n",
    "    print(\"\\nProbabilities:\")\n",
    "    for i in range(3):\n",
    "        print(f\"- {label_names[i]}: {probs[i] * 100:.1f}%\")\n",
    "    print(\"=\" * 50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532f2011",
   "metadata": {},
   "source": [
    "Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51e37596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Input: 'quel est l'etat de ma commande?'\n",
      "Predicted: Suivi (Tracking) (confidence: 100.0%)\n",
      "\n",
      "Probabilities:\n",
      "- Suivi (Tracking): 100.0%\n",
      "- R√©clamation (Complaint): 0.0%\n",
      "- Commande (Order): 0.0%\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classify_input(\"quel est l'etat de ma commande?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526085e6",
   "metadata": {},
   "source": [
    "Prompt Classification (zero shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "984dfeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: Je veux acheter Doliprane\n",
      "Predicted intent: suivi (92.88%)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# List of all 60 intent labels (here's a shortened version ‚Äî use full in practice)\n",
    "intent_labels = [\n",
    "    \"suivi\", \"commande\", \"reclamation\"\n",
    "]\n",
    "\n",
    "# Load the model\n",
    "classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"cartesinus/multilingual_minilm-amazon_massive-intent_eu7\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Define a simple function for classification\n",
    "def predict_intent(text):\n",
    "    result = classifier(text, intent_labels, multi_label=False)\n",
    "    return result['labels'][0], result['scores'][0]\n",
    "\n",
    "# Example usage\n",
    "message = \"Je veux acheter Doliprane\"\n",
    "intent, confidence = predict_intent(message)\n",
    "print(f\"Message: {message}\")\n",
    "print(f\"Predicted intent: {intent} ({confidence:.2%})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "268ab9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ou est ma commande?\n",
      "Predicted intent: takeaway_query\n",
      "Confidence: 99.53%\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# pour le traitement tensoriel\n",
    "import torch \n",
    "# pour transformer les logits en probabilit√©s.\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"cartesinus/multilingual_minilm-amazon_massive-intent_eu7\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Put model in evaluation mode (d√©sactive dropout, batch norm, etc.)\n",
    "model.eval()\n",
    "\n",
    "# Define the text\n",
    "text = \"ou est ma commande?\"\n",
    "\n",
    "# Tokenize the input\n",
    "# tokenisation avec ajout automatique de batch\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass\n",
    "# d√©sactive le calcul du gradient (no_grad) car ce n‚Äôest pas un entra√Ænement.\n",
    "# logits: sorties brutes du mod√®le.\n",
    "# softmax: convertit les logits en probabilit√©s de chaque classe\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "\n",
    "# Get predicted class index\n",
    "# S√©lectionne l‚Äôindice de la classe avec la probabilit√© la plus √©lev√©e et R√©cup√®rer le score de confiance.\n",
    "predicted_index = torch.argmax(probs, dim=1).item()\n",
    "confidence = torch.max(probs).item()\n",
    "\n",
    "# Load class labels\n",
    "id2label = model.config.id2label\n",
    "predicted_label = id2label[predicted_index]\n",
    "\n",
    "# Print result\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Predicted intent: {predicted_label}\")\n",
    "print(f\"Confidence: {confidence:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80350c01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
