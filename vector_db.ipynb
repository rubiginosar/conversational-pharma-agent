{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cc4a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FAISS index created and saved to disk.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# 1. Set up HuggingFace embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# 2. Load your pharmaceutical data\n",
    "df = pd.read_csv(\"data/product.csv\", sep=\";\", encoding=\"ISO-8859-1\")\n",
    "\n",
    "# 3. Convert each row to a LangChain Document\n",
    "raw_documents = []\n",
    "ids = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    content = f\"\"\"\n",
    "    ProductId: {row['ProductId']}\n",
    "    Product Name: {row['ProductName']}\n",
    "    DCI: {row['DCI']}\n",
    "    Dosage: {row['Dosage']}\n",
    "    Form: {row['FormId']}\n",
    "    Brand: {row['Brand']}\n",
    "    Laboratory: {row['Laboratory']}\n",
    "    Pharmacological Class: {row['PharmacologicalClass']}\n",
    "    Therapeutic Class: {row['TherapeuticClass']}\n",
    "    Refundable: {row['Refundable']}\n",
    "    Price: {row['Ppa']}\n",
    "    Stock: {row['MaxThreshold']}\n",
    "    \"\"\"\n",
    "    doc = Document(\n",
    "        page_content=content.strip(),\n",
    "        metadata={\n",
    "            # \"ProductId\": row[\"ProductId\"],\n",
    "            \"Ppa\": row[\"Ppa\"],\n",
    "            \"Stock\": row[\"MaxThreshold\"],\n",
    "            \"DCI\": row[\"DCI\"],\n",
    "            \"ProductName\": row[\"ProductName\"]\n",
    "        }\n",
    "    )\n",
    "    raw_documents.append(doc)\n",
    "    ids.append(str(i))\n",
    "\n",
    "# 4. Split documents for embedding\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = splitter.split_documents(raw_documents)\n",
    "\n",
    "# 5. Path to save/load FAISS index\n",
    "index_path = \"vectorial_dbs/faiss_index_\"\n",
    "\n",
    "# 6. Load or build FAISS vector store\n",
    "if os.path.exists(index_path):\n",
    "    vector = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "    print(\"✅ FAISS index loaded from disk.\")\n",
    "else:\n",
    "    vector = FAISS.from_documents(documents, embeddings)\n",
    "    vector.save_local(index_path)\n",
    "    print(\"✅ FAISS index created and saved to disk.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e194fbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FAISS index created and saved to disk.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# 1. Set up HuggingFace embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# 2. Load the orderlines_with_status.csv and normalize OrderDate to ISO format\n",
    "df_orders = pd.read_csv(\"data/orderlines_with_status.csv\")\n",
    "df_orders['OrderDate'] = pd.to_datetime(df_orders['OrderDate']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# 3. Convert each row to a LangChain Document\n",
    "order_documents = []\n",
    "\n",
    "for _, row in df_orders.iterrows():\n",
    "    content = f\"\"\"\n",
    "    Order ID: {row['RefOrderId']}\n",
    "    Product: {row['ProductName']}\n",
    "    Quantity: {row['Qty']}\n",
    "    Amount: {row['TotalAmount']}\n",
    "    Order Date: {row['OrderDate']}\n",
    "    Status: {row['status']}\n",
    "    \"\"\"\n",
    "    doc = Document(\n",
    "        page_content=content.strip(),\n",
    "        metadata={\n",
    "            \"order_id\": str(row[\"RefOrderId\"]),\n",
    "            \"product_name\": row[\"ProductName\"],\n",
    "            \"quantity\": str(row[\"Qty\"]),\n",
    "            \"status\": row[\"status\"],\n",
    "            \"order_date\": row[\"OrderDate\"],\n",
    "        }\n",
    "    )\n",
    "    order_documents.append(doc)\n",
    "\n",
    "# 4. Optional: Split long documents (not strictly necessary unless content is large)\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "order_documents_split = splitter.split_documents(order_documents)\n",
    "\n",
    "# 5. Define path to save/load FAISS index\n",
    "index_path = \"vectorial_dbs/faiss_index_order\"\n",
    "\n",
    "# 6. Load or build FAISS vector store\n",
    "if os.path.exists(index_path):\n",
    "    vector = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "    print(\"✅ FAISS index loaded from disk.\")\n",
    "else:\n",
    "    vector = FAISS.from_documents(order_documents_split, embeddings)\n",
    "    vector.save_local(index_path)\n",
    "    print(\"✅ FAISS index created and saved to disk.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
